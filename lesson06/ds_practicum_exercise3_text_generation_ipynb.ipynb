{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3XPuaTu9jl"
      },
      "source": [
        "\n",
        "# Text generation - using different decoding methods\n",
        "\n",
        "Simplified from: https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb\n",
        "\n",
        "We will use the [GPT2 model](https://huggingface.co/gpt2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1f18e3-7ac9-4c09-ca55-8fbf6903f4ec"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'gpt2'\n",
        "MAX_LEN = 45"
      ],
      "metadata": {
        "id": "rVH48j1nYQnC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kOQhXTAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03197d5-8e8d-4db4-a430-fe4ec4cdd73d"
      },
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
        "model = TFGPT2LMHeadModel.from_pretrained(MODEL, pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Y7cgu9ohXP"
      },
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$.\n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the context that conditions the generation:"
      ],
      "metadata": {
        "id": "i_8RYTfYY6lk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLd_J6lXz_t",
        "outputId": "6e1de372-c587-4622-9ba2-81f46bd79335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_ids = tokenizer.encode('The best way to spend your time on the weekend is', return_tensors='pt')\n",
        "input_ids"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 464, 1266,  835,  284, 4341,  534,  640,  319,  262, 5041,  318]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text until the output length *(which includes the context length)* reaches `MAX_LEN`:"
      ],
      "metadata": {
        "id": "-gIgmnmWY88w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_output = model.generate(input_ids, max_length=MAX_LEN)\n",
        "greedy_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqE18oF4Y3-5",
        "outputId": "f72ab943-9930-4af4-d0b4-a9bf403b6a51"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 45), dtype=int32, numpy=\n",
              "array([[  464,  1266,   835,   284,  4341,   534,   640,   319,   262,\n",
              "         5041,   318,   284,   467,   284,   257,  1957,  2318,   393,\n",
              "         7072,   290,  1949,   617,   286,   262,  1266, 34281,   287,\n",
              "         3240,    13,   198,   198,   464,  1266,   835,   284,  4341,\n",
              "          534,   640,   319,   262,  5041,   318,   284,   467,   284]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decode the generated ids to words:"
      ],
      "metadata": {
        "id": "cKn6N7_lZC6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bty510GYY4Go",
        "outputId": "641b5d17-f775-4edf-e8e2-c141ec7305c8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The best way to spend your time on the weekend is to go to a local bar or restaurant and try some of the best cocktails in town.\\n\\nThe best way to spend your time on the weekend is to go to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBn1ePmJvhrl"
      },
      "source": [
        "The generated words after context are reasonable, but the model quickly starts repeating itself - common problem in language generation ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) or [Shao et al., 2017](https://arxiv.org/abs/1701.03185)).\n",
        "\n",
        "Major drawback of greedy search: it misses high probability words hidden behind a low probability word (word \"has\" after \"dog\").\n",
        "\n",
        "=> beam search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8DnXZ1WiuNd"
      },
      "source": [
        "### **Beam search**\n",
        "\n",
        "Reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.\n",
        "\n",
        "For `num_beams=2`:\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "1. Time step: besides the most likely hypothesis $\\text{\"The\", \"nice\"}$, beam search also keeps track of the second most likely one $\\text{\"The\", \"dog\"}$.\n",
        "\n",
        "2. Time step: beam search finds that the word sequence $\\text{\"The\", \"dog\", \"has\"}$ has with $0.36$ a higher probability than $\\text{\"The\", \"nice\", \"woman\"}$, which has $0.2$.\n",
        "\n",
        "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1R5kx30Ynej",
        "outputId": "0da3195c-edc6-4dc9-e39a-e7d0ba1a8992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# activate the beam search by setting num_beams > 1\n",
        "\n",
        "# early_stopping=True => the generation is finished when all beam hypotheses\n",
        "# reached the EOS token\n",
        "\n",
        "# n-gram (word sequences of n words) penalties:\n",
        "# no_repeat_ngram_size=2 => no 2-gram appears twice\n",
        "\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=MAX_LEN,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "tokenizer.decode(beam_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The best way to spend your time on the weekend is to take a break from work and spend time with your family, friends, and loved ones.\\n\\nIf you have any questions, please feel free to reach out to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsksOGDpmA0"
      },
      "source": [
        "Repetition does not appear anymore.\n",
        "\n",
        "Nevertheless, *n-gram* penalties have to be used with care. An article generated about the city *New York* should not use a *2-gram* penalty or otherwise, the name of the city would only appear once in the whole text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbbIyK84wHq6"
      },
      "source": [
        "### **Sampling**\n",
        "\n",
        "Randomly pick the next word $w_t$ according to its conditional probability distribution:\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "Taking the example from above, the following graphic visualizes language generation when sampling.\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "(Language generation using sampling is not *deterministic* anymore. The word\n",
        "$\\text{\"car\"}$ is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling $\\text{\"drives\"}$ from $P(w | \\text{\"The\"}, \\text{\"car\"})$.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputId": "4fc5ed84-69a0-4726-c181-bd58c67c90d6",
        "id": "aRAz4D-Ks0_4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# activate sampling (do_sample=True) and deactivate top_k sampling (top_k=0)\n",
        "\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=MAX_LEN,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The best way to spend your time on the weekend is to drop by your local Jenks cafe. And if you're in every hotel, you can choose from a range of hand-crafted and hand stocked South Asian clothing to\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQHuo911wfT-"
      },
      "source": [
        "A common problem when sampling word sequences: The models often generate incoherent gibberish, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n",
        "\n",
        "A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max).\n",
        "\n",
        "An illustration of applying temperature to our example from above could look as follows.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
        "\n",
        "The conditional next word distribution of step $t=1$ becomes much sharper leaving almost no chance for word $\\text{\"car\"}$ to be selected.\n",
        "\n",
        "\n",
        "Let's see how we can cool down the distribution in the library by setting `temperature=0.7`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgJredc-0j0Z",
        "outputId": "591750cb-d079-41e9-c103-7d4ce5671d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=MAX_LEN,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best way to spend your time on the weekend is to get out of your car and walk to a nice place to eat and relax.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzGuu24hZZnq"
      },
      "source": [
        "The output is a bit more coherent now.\n",
        "\n",
        "While applying temperature can make a distribution less random, in its limit, when setting `temperature` $ \\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binNTroyzQBu"
      },
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "The *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only *ca.* two-thirds of the whole probability mass in the first step, it includes almost all of the probability mass in the second step. Nevertheless, we see that it successfully eliminates the rather weird candidates $\\text{\"not\", \"the\", \"small\", \"told\"}$\n",
        "in the second sampling step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBtDOdD0wx3l",
        "outputId": "550c56c0-6d76-4cae-9f81-820b51b4e797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=MAX_LEN,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The best way to spend your time on the weekend is to check out what life is like at the top-notch university in China or Shanghai. You can also meet some of the best Chinese students, teachers, students,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77H5m4ZmhEX"
      },
      "source": [
        "One concern with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$ => can be problematic as some words might be sampled from a very sharp distribution (right on graph above), whereas others from a much more flat distribution (left).\n",
        "\n",
        "In step $t=1$, *Top-K* eliminates the possibility to\n",
        "sample $\\text{\"people\", \"big\", \"house\", \"cat\"}$, which seem like reasonable candidates. On the other hand, in step $t=2$ the method includes the arguably ill-fitted words $\\text{\"down\", \"a\"}$ in the sample pool of words. Thus, limiting the sample pool to a fixed size *K* could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution.\n",
        "This intuition led [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) to create ***Top-p***- or ***nucleus***-sampling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9LAaexzV3H"
      },
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "Instead of sampling only from the most likely *K* words, we choose from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words => the size of the set of words can dynamically increase and decrease according to the next word's probability distribution.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92\\%$ of the probability mass, defined as $V_{\\text{top-p}}$. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvwIc7YAx77F",
        "outputId": "1a1a7b40-baf7-4cb8-88f7-6ef2e75ede01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=MAX_LEN,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "tokenizer.decode(sample_output[0], skip_special_tokens=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The best way to spend your time on the weekend is with friends and family. Caring for yourself and creating and recording music is a rewarding way to spend a few minutes at the gym each day.\\n\\n– Pray'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn-8gLaR4lat"
      },
      "source": [
        "*Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some dynamic selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kY8P9VG8Gi9",
        "outputId": "6fe476be-708e-494d-9307-61f520ee28c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=MAX_LEN,\n",
        "    top_k=10,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "tokenizer.decode(sample_output, skip_special_tokens=True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The best way to spend your time on the weekend is to get a few friends and family together and watch TV. I've heard of other people who do this on a Friday or Saturday morning and they do it in their spare\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4CYi91h11yd"
      },
      "source": [
        "### Additional parameters\n",
        "\n",
        "- `min_length` - to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached, *used quite frequently in summarization*\n",
        "- `repetition_penalty` - to penalize words that were already generated or belong to the context\n",
        "- `attention_mask` - can be used to mask padded tokens\n",
        "- `pad_token_id`, `bos_token_id`, `eos_token_id` - if the model does not have those tokens by default, the user can manually choose other token ids to represent them\n",
        "\n",
        "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
      ]
    }
  ]
}